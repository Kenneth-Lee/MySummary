.. Kenneth Lee 版权所有 2025

:Authors: Kenneth Lee
:Version: 0.1
:Date: 2025-11-10
:Status: Draft

Embedding
*********

我最近在“机器学习”相关业务学习中，从不同的上下文中了解到Embedding这个概念，对
它有了一个较为自洽的认识，我把这些信息综合起来，总结一下，写在本文中。

按geeksforgeeks.org的一个解释，Embedding是：

  | In machine learning, the term "embeddings" refers to a method of
  | transforming high-dimensional data into a lower-dimensional space while
  | preserving essential relationships and properties.

可以有单词的Embedding，句子，图像，图形，音乐……等等等等的Embedding。中文常常翻
译为“嵌入”，我觉得表达的是：这个东西地被植入到全部信息中了。

我们用一个具体的例子解释一下。

比如，你有一幅1024x768的图，每个点用一个数字表示，这是一个包含786432个数字的向
量，这里有很多信息，但我们可以用一个更小的向量去表达它，这个向量“总结”了整副图，
我们就把这个向量称为这幅图的Embedding，说的就是，我们新的这组数字，不是原来图
上的任何一个点，而是“嵌入”到了这幅图的方方面面的一个“总结”，是这幅图的一个
Embedding。

Embedding在表面上，不见得就一定比它总结的对象维度少，比如LLM中，每个Token（单
词）都用一个很大的向量表示（比如和单词表一样多维），这也是一个Embedding。类似
这样：::

  单词       Embedding
  你    =>   (0., 0., 0.3, 0.7, 0., 0., ....)
  好    =>   (0.1, 0., 0., 0.2, 0.4, 0, ....)

它可以表示这个单词（字）和单词表中每个其他单词的相关性。这也是Embedding。表面
上，一个单词的信息量好像就是它的字符编码。但在自然语言中，一个单词的信息量，在
单独使用的时候，是它和所有其他词的关系。所以，我们前面强调说“表面上”Embedding
可以比它表示的对象更高维，但实际上，这完全看你怎么看这个问题，一般我们还是认为，
原始的对象包含更多的信息的，所以，就有了前面那个定义的说法：这通常是一个高维向
低维的转换。

这是纯数学的看法。用人脑更容易理解的方式，其实Embedding主要可以类比我们人脑认
识的“换角度思考”。比如我们看到一幅画，我们不会认知这幅画的每个点，我们会换一个
角度这样认识它：[主角，色彩风格，画风，题材，……]。这幅画的所有点，都对这个新组
织的维度有不同的贡献。

这个从[0/0w位置的颜色,0/1位置的颜色，0/2位置的颜色...]到[主角，色彩风格，画风，
题材，……]的变换过程，最终就是神经网络里面的一个“层”：输入向量对权重做矩阵做一
个矩阵乘法再加一个加激活函数作非线性化。而所谓矩阵乘法，就是一个线性变维，通过
一组权重，把原来N维的向量变维成M维的向量。类比前面把表示1024x768的图的786432维
向量，转化为比如1024维的向量。

各种神经网络，都是在不断做这种转维，尝试让原来表面没什么规律的东西，“换个角度”
看看它有没有规律。变维以后，有些规律是可以和我们平时总结的“特征”对应的，这种我
们称为“显式”特征，比如前面提到的“主角”，“色彩风格”，这些和我们平时认知的东西是
“可比”的，就是显式的特征。

有些，我们是觉得它是有规律的，但这个内部的规律没法和我们说惯的特征一一对应，这
种特征我们就叫“隐式”特征。反正它就是有个数字表示，可以用来表示某个角度的规律，
但你非要说它是什么，我们没有对应的概念来表示它。

神经网络的多个层，一层层做转维，不断训练每层的权重矩阵，然后不断输出后面不同层
之间的向量，说到底就是一个个隐式的Embedding。所有这些Embedding，都“代表”同一个
对象，只是不同维的表达而已。

这就好像我们平时说一个人，我们可能从相亲的角度去权衡他，可能从招工的角度去权衡
他，可能从战斗的角度去权衡他，我们就有不同的Embedding去表征他了，但我们说的还
是同一个人。这个人就是维特根斯坦说的Thing，道德经说的“道”，但每个Embedding，都
只是这个本体的“Statement”或者“名”。

Embedding在数学上最大的优势就在于它的转维是一个并行计算，可以“立即出结果”。在
生物上，多个感应器按不同的强度（权重）传递到下一个神经元，直接刺激这个神经元，
而在机器学习而时候，每个权重乘法可以被GPU的多个通道并行计算，仅仅是最终汇总结
果的时候需要串行全部加起来。这比一般的逻辑那种“因为a所以b，因为b，所以c……”这样
的串行逻辑快多了。

同样，两个同维（不单指维度数量相同，而是每个维度代表的是同一个特征）的向量也很
容易并行比较“距离”。就好像二维空间的两个点的距离，这个距离可以分到每维去并行计
算，然后简单汇总就行了。这就是我们人脑能看到某种东西很快就能“联想”到它是某种东
西的原因。我们是通过一眼看过去，就直接在不同角度上比较不同Embedding的距离，从
而“觉得”它像是什么。这不需要经过逻辑思考的，因为这是并行计算一次生成的结果。

现在RAG技术就是这样的：我们先按某种特征“总结”很多的论文，用一些Embedding表示这
篇论文，然后我们用一段文字去表达我们关心的内容，然后也生成同维的Embedding，然
后我们用这个Embedding去查找我们的论文数据库，快速匹配出和我们期望的论文。甚至
用这些论文作为上下文，再去回答我们一开始询问的问题。

电商的推荐系统现在也是这样的：电商的商品，无论是实物商品，还是数字商品，都可以
训练出一组特征，用Embedding来表示。你上电商的网站，点击了一连串商品，加上你的
个人信息，也可以训练出一个同维的Embedding，然后电商进行快速匹配，在几十毫秒只
能找出几个最接近的广告推荐给你。

这个概念可以泛化到所有需要在大量数据中迅速定位特定对象的系统中。从这个角度来说，
这一波人工智能不需要像Richard Sutton说的那样，非要走强化学习路线，让人工智能变
成人，让它有自我发展和学习能力。它只需要变成充分展现不同角度的Embedding的大型
数据库，从而变成人脑真正的扩充，它就会变成未来人类的一个知识扩充，成为我们现在
觉得顺理成章的各种水电，机械，外卖，搜索引擎……一样，每个人都离不开的东西。

说到电商的推荐算法，我在一两周前其实几乎没有认识的。我只读过一篇论文，知道它是
解决什么问题的。这两周我分析一段电商代码的性能优化，里面又是稀疏查询，又是稠密
查询，又用到pytorch.Embedding模块处理多个特征。按我以前的经验，我要理解这个代
码至少要两三个月。但实际上我只花了几天，通过直接把部分代码给它看，让它“猜”：在
推荐系统中，这样写的代码是干什么用的，让它解释为什么要分成稀疏查询和稠密查询来
获得最终的推荐，然后我就在很短时间掌握这段代码的主体逻辑了。

这实在是惊艳到我了，因为之前我尝试很很多次用AI来辅助写代码，但在逻辑复杂了之后，
它的帮助就非常有限了（甚至变成负担）。但如果换个角度，把它看作是一个咨询系统，
给它人分离好的逻辑，让它单独解决某个具体的问题，它的效果会非常好。你别给它那么
多信息让它自己创造，你自己先有一个逻辑框架，然后直接让他校验你的关键总结，比如
“推荐系统保存Embedding可以用什么数据库？”，或者“如果在稠密匹配前不使用稀疏转换
的结果是什么？请举例说明。”……这样它就能一步步帮你接近你的分析和决策结论。

换句话说，它不能代替你学习，也不能代替你完成全部的工作，但它确实让你不用记忆很
多细节，而且在一个复杂的信息系统中快速接近你要达成的目标。我自己当架构师的，要
看的代码非常多，你让我记住那么多代码的细节，我记得后面的就会忘掉前面的，但有了
这个辅助后，我可以放心忘记很多细节，我记住整体逻辑就行了。这种思维模型我认为甚
至慢慢会改变人类的。

我觉得不会使用搜索引擎的人在现代的学术研究中会被淘汰，同样的，下一波不知道怎么
用AI这种搜索引擎的人在现代的学术研究中也会被淘汰。因为它不需要解决幻觉的问题，
不需要能强化学习变成人，它就已经是一个强力的，多角度可以帮你匹配信息的查询工具，
是我们的扩展大脑，人类慢慢会离不开它的。
